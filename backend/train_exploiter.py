import torch
import torch.optim as optim
import torch.nn.functional as F
from models.muzero_nexus import MuZeroNexus
from game_gym import RiftboundEnv
from league.league_manager import LeagueManager
import numpy as np
import os

class ExploiterTrainer:
    """
    Trains an 'Exploiter' agent to beat the fixed MainAgent.
    Uses Minimax Reward: R_exploiter = R_env - alpha * V_opponent
    """
    def __init__(self, main_agent_path="backend/models/muzero_main_v1.pt"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Initialize Exploiter from MainAgent weights (Hot-start)
        self.exploiter = MuZeroNexus().to(self.device)
        self.league = LeagueManager(main_agent_path)
        
        if os.path.exists(main_agent_path):
            try:
                self.exploiter.load_state_dict(torch.load(main_agent_path, map_location=self.device))
                print("Exploiter initialized with MainAgent weights.")
            except:
                print("Warning: Failed to load MainAgent weights for Exploiter. Random init.")
        else:
            print("Warning: MainAgent not found. Exploiter starting from scratch.")

        self.optimizer = optim.Adam(self.exploiter.parameters(), lr=1e-3)
        self.env = RiftboundEnv()

    def train_step_minimax(self, obs, action, next_obs, env_reward, alpha=0.1):
        self.exploiter.train()
        self.optimizer.zero_grad()
        
        # 1. Standard MuZero Loss for Exploiter
        obs_t = torch.tensor(obs).unsqueeze(0).to(self.device)
        next_obs_t = torch.tensor(next_obs).unsqueeze(0).to(self.device)
        action_t = torch.tensor([float(action)]).to(self.device)
        
        s0 = self.exploiter.representation(obs_t)
        p, v = self.exploiter.prediction(s0)
        s1_pred, r_pred = self.exploiter.next_step(s0, action_t)
        
        # 2. Calculate Minimax Target
        # V_opponent(s') -> How good does opponent think the NEXT state is?
        v_opp = self.league.get_opponent_value(next_obs)
        
        # Maximizing R_env AND Minimizing V_opp (Opponent confidence)
        minimax_reward = env_reward - (alpha * v_opp)
        target_reward = torch.tensor([[minimax_reward]]).to(self.device)
        
        # Loss Components
        o_hat = self.exploiter.reconstruct(s0)
        l_recon = F.mse_loss(o_hat, obs_t)
        
        with torch.no_grad():
            s1_target = self.exploiter.representation(next_obs_t)
            
        l_dynamics = F.mse_loss(s1_pred, s1_target)
        l_reward = F.mse_loss(r_pred, target_reward)
        
        total_loss = l_dynamics + l_reward + l_recon
        total_loss.backward()
        self.optimizer.step()
        
        return total_loss.item()

    def run_breaking_run(self, episodes=50):
        print(f"Starting 'The Breaking Run' (Exploiter Training) with alpha=0.1...")
        wins = 0
        
        for ep in range(episodes):
            obs, info = self.env.reset()
            terminated = False
            steps = 0
            
            while not terminated and steps < 50:
                steps += 1
                
                # Exploiter Move
                s_exp = self.exploiter.representation(torch.tensor(obs).unsqueeze(0).to(self.device))
                p_exp, _ = self.exploiter.prediction(s_exp)
                action_idx = torch.argmax(p_exp[0]).item() 
                
                # Step (Against Environment / Simulating Opponent)
                next_obs, reward, terminated, truncated, info = self.env.step(action_idx)
                
                # Train
                loss = self.train_step_minimax(obs, action_idx, next_obs, reward, alpha=0.1)
                obs = next_obs
                
                if terminated and reward > 0:
                    wins += 1
            
            if ep % 10 == 0:
                print(f"Episode {ep} | Loss: {loss:.4f} | WR: {wins/(ep+1):.2f}")

        print(f"Breaking Run Complete. Final WR: {wins/episodes:.2f}")
        
        # Save Exploiter
        os.makedirs("backend/models", exist_ok=True)
        torch.save(self.exploiter.state_dict(), "backend/models/muzero_exploiter_v1.pt")
        print("Exploiter_v1 saved.")

if __name__ == "__main__":
    trainer = ExploiterTrainer()
    trainer.run_breaking_run(episodes=50)
