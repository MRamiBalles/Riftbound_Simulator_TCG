# -*- coding: utf-8 -*-
"""
Entrenamiento de Agente Explotador (ROA-Star League)
====================================================

Entrena un agente "Exploiter" diseñado para descubrir y explotar debilidades
en la estrategia del agente principal (MainAgent).

Fundamento teórico:
El entrenamiento de liga (League Training) usado en AlphaStar mantiene una
población de agentes especializados. Los explotadores se recompensan por
ganar contra el agente principal, forzándolo a desarrollar estrategias robustas.

Función de recompensa Minimax:
    R_exploiter = R_env - α · V_opponent(s')

Donde:
- R_env: Recompensa del entorno (victoria/derrota)
- V_opponent: Valor que el oponente asigna al siguiente estado
- α: Factor de ponderación (0.1 por defecto, evita comportamiento "kamikaze")

Al restar el valor del oponente, el explotador aprende a buscar estados
donde el oponente tiene baja confianza, indicando posibles puntos ciegos.

Author: Manuel Ramirez Ballesteros
Email: ramiballes96@gmail.com
Copyright (c) 2026 Manuel Ramirez Ballesteros. All rights reserved.
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
from models.muzero_nexus import MuZeroNexus
from game_gym import RiftboundEnv
from league.league_manager import LeagueManager
import numpy as np
import os


class ExploiterTrainer:
    """Entrenador del agente explotador.
    
    Inicializa el explotador con los pesos del MainAgent (hot-start)
    y lo entrena usando la recompensa Minimax para encontrar contra-estrategias.
    """
    
    def __init__(self, main_agent_path: str = "backend/models/muzero_main_v1.pt"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Inicializar explotador desde MainAgent (hot-start)
        self.exploiter = MuZeroNexus().to(self.device)
        self.league = LeagueManager(main_agent_path)
        
        if os.path.exists(main_agent_path):
            try:
                self.exploiter.load_state_dict(
                    torch.load(main_agent_path, map_location=self.device)
                )
                print("Explotador inicializado con pesos de MainAgent.")
            except Exception as e:
                print(f"Advertencia: Error cargando pesos ({e}). Inicialización aleatoria.")
        else:
            print("Advertencia: MainAgent no encontrado. Iniciando desde cero.")

        self.optimizer = optim.Adam(self.exploiter.parameters(), lr=1e-3)
        self.env = RiftboundEnv()

    def train_step_minimax(
        self, 
        obs: np.ndarray, 
        action: int, 
        next_obs: np.ndarray, 
        env_reward: float, 
        alpha: float = 0.1
    ) -> float:
        """Ejecuta un paso de entrenamiento con recompensa Minimax.
        
        Args:
            obs: Observación actual
            action: Acción ejecutada
            next_obs: Observación resultante
            env_reward: Recompensa del entorno
            alpha: Peso del término Minimax (0.1 evita comportamiento extremo)
        
        Returns:
            Valor de la pérdida
        """
        self.exploiter.train()
        self.optimizer.zero_grad()
        
        # Convertir observaciones a tensores
        obs_t = torch.tensor(obs).unsqueeze(0).to(self.device)
        next_obs_t = torch.tensor(next_obs).unsqueeze(0).to(self.device)
        action_t = torch.tensor([float(action)]).to(self.device)
        
        # Forward del explotador
        s0 = self.exploiter.representation(obs_t)
        policy, value = self.exploiter.prediction_net(s0)
        s1_pred, r_pred = self.exploiter.next_step(s0, action_t)
        
        # Obtener valor del oponente para el siguiente estado
        v_opponent = self.league.get_opponent_value(next_obs)
        
        # Recompensa Minimax: maximizar R_env y minimizar confianza del oponente
        minimax_reward = env_reward - (alpha * v_opponent)
        target_reward = torch.tensor([[minimax_reward]]).to(self.device)
        
        # Componentes de pérdida
        o_hat = self.exploiter.reconstruct(s0)
        l_recon = F.mse_loss(o_hat, obs_t)
        
        with torch.no_grad():
            s1_target = self.exploiter.representation(next_obs_t)
            
        l_dynamics = F.mse_loss(s1_pred, s1_target)
        l_reward = F.mse_loss(r_pred, target_reward)
        
        total_loss = l_dynamics + l_reward + l_recon
        total_loss.backward()
        self.optimizer.step()
        
        return total_loss.item()

    def run_breaking_run(self, episodes: int = 50) -> None:
        """Ejecuta el entrenamiento adversarial ("The Breaking Run").
        
        El explotador juega múltiples partidas contra el agente principal,
        aprendiendo a encontrar y explotar sus debilidades.
        """
        print(f"Iniciando 'The Breaking Run' con alpha=0.1...")
        wins = 0
        
        for ep in range(episodes):
            obs, info = self.env.reset()
            terminated = False
            steps = 0
            loss = 0.0
            
            while not terminated and steps < 50:
                steps += 1
                
                # Selección de acción del explotador
                with torch.no_grad():
                    s_exp = self.exploiter.representation(
                        torch.tensor(obs).unsqueeze(0).to(self.device)
                    )
                    p_exp, _ = self.exploiter.prediction_net(s_exp)
                    action_idx = torch.argmax(p_exp[0]).item()
                
                # Ejecutar acción
                next_obs, reward, terminated, truncated, info = self.env.step(action_idx)
                
                # Entrenar
                loss = self.train_step_minimax(obs, action_idx, next_obs, reward, alpha=0.1)
                obs = next_obs
                
                if terminated and reward > 0:
                    wins += 1
            
            if ep % 10 == 0:
                wr = wins / (ep + 1)
                print(f"Episodio {ep} | Pérdida: {loss:.4f} | Victorias: {wr:.2f}")

        final_wr = wins / episodes
        print(f"Breaking Run completado. Tasa de victorias: {final_wr:.2f}")
        
        # Guardar modelo entrenado
        os.makedirs("backend/models", exist_ok=True)
        torch.save(
            self.exploiter.state_dict(), 
            "backend/models/muzero_exploiter_v1.pt"
        )
        print("Exploiter_v1 guardado.")


if __name__ == "__main__":
    trainer = ExploiterTrainer()
    trainer.run_breaking_run(episodes=50)
